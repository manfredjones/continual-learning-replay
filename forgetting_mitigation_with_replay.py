# -*- coding: utf-8 -*-
"""Forgetting Mitigation with Replay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-JQLwsWq12xFTrBg0jpH2B0Uckzi0_iB
"""

!pip install transformers datasets evaluate sentencepiece rouge_score

from google.colab import files

import pandas as pd

# Cargar Dataset A (simplificación)
df_a = pd.read_excel('simplificacion.xlsx', usecols=["Segmento", "Propuesta"])

# Ver cuántos ejemplos hay
print(f"Tamaño Dataset A: {len(df_a)}")
df_a.head()

df_a["prefix"] = "simplify: "
df_a["input_text"] = df_a["prefix"] + df_a["Segmento"]
df_a["target_text"] = df_a["Propuesta"]

from sklearn.model_selection import train_test_split

df_a_train, df_a_test = train_test_split(df_a, test_size=0.2, random_state=42)

print(f"Train A: {len(df_a_train)}, Test A: {len(df_a_test)}")






# Cargar Dataset B (traducción)
df_b = pd.read_excel('traduccion.xlsx', usecols=["Segmento", "Propuesta"])

print(f"Tamaño Dataset B: {len(df_b)}")
df_b.head()


# LIMPIAR
# Eliminar filas donde Propuesta es float
df_b = df_b[df_b["Propuesta"].apply(lambda x: isinstance(x, str))]

# Dropear posibles NaN
df_b = df_b.dropna(subset=["Segmento", "Propuesta"])

# Asegurar que todo es string
df_b["Segmento"] = df_b["Segmento"].astype(str)
df_b["Propuesta"] = df_b["Propuesta"].astype(str)


df_b["prefix"] = "translate"
df_b["input_text"] = df_b["prefix"] + df_b["Segmento"]
df_b["target_text"] = df_b["Propuesta"]

df_b_train, df_b_test = train_test_split(df_b, test_size=0.2, random_state=42)

print(f"Train B: {len(df_b_train)}, Test B: {len(df_b_test)}")

from torch.utils.data import Dataset
import torch

class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        input_text = self.dataframe.iloc[idx]["input_text"]
        target_text = self.dataframe.iloc[idx]["target_text"]

        input_enc = self.tokenizer(
            input_text,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        target_enc = self.tokenizer(
            target_text,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": input_enc["input_ids"].squeeze(),
            "attention_mask": input_enc["attention_mask"].squeeze(),
            "labels": target_enc["input_ids"].squeeze()
        }

from transformers import T5ForConditionalGeneration, T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

train_dataset_a = TextDataset(df_a_train, tokenizer)
test_dataset_a = TextDataset(df_a_test, tokenizer)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir='./results_A',
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_dir='./logs_A',
    logging_steps=10,
    learning_rate=5e-4,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_a,
    eval_dataset=test_dataset_a
)

trainer.train()

# Guardar el modelo
model.save_pretrained("./model_after_A")
tokenizer.save_pretrained("./model_after_A")

import pandas as pd
# Lista para almacenar ejemplos
examples_manual = []

device = "cuda" if torch.cuda.is_available() else "cpu"


# Mover el modelo a device
model.to(device)

def simplify_text(text):
    input_text = "simplify: " + text
    input_enc = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=128).to(device)
    output = model.generate(input_enc, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Generar ejemplos
for idx in range(5):  # o 10, lo que quieras
    original_text = df_a_test.iloc[idx]["Segmento"]
    target_text = df_a_test.iloc[idx]["Propuesta"]
    prediction = simplify_text(original_text)

    input_text = "simplify: " + original_text

    # Guardar ejemplo como dict
    examples_manual.append({
        "N°": idx+1,
        "Input (prefix + texto)": input_text,
        "Target": target_text,
        "Prediction": prediction
    })

    # También imprimir por consola (opcional)
    print(f"Ejemplo {idx+1}")
    print("Input:", input_text)
    print("Target:", target_text)
    print("Prediction:", prediction)
    print("-" * 80)

import evaluate
rouge = evaluate.load("rouge")

# Crear listas de predictions y references
predictions = []
references = []

for i in range(len(df_a_test)):
    input_text = df_a_test.iloc[i]["Segmento"]
    target_text = df_a_test.iloc[i]["Propuesta"]

    pred_text = simplify_text(input_text)

    predictions.append(pred_text)
    references.append(target_text)

# Calcular ROUGE
results = rouge.compute(predictions=predictions, references=references)

print(results)

bleu = evaluate.load("bleu")

# BLEU espera referencias como listas de listas
references_bleu = [[ref] for ref in references]

results_bleu = bleu.compute(predictions=predictions, references=references_bleu)

print(results_bleu)

import pandas as pd

# Crear DataFrame con las métricas
df_metrics = pd.DataFrame({
    "Metric": ["ROUGE-1", "ROUGE-2", "ROUGE-L", "ROUGE-Lsum", "BLEU"],
    "Score": [
        results["rouge1"],
        results["rouge2"],
        results["rougeL"],
        results["rougeLsum"],
        results_bleu["bleu"]
    ]
})

# Mostrar la tabla
df_metrics

import matplotlib.pyplot as plt

# Filtrar solo métricas ROUGE
df_rouge_metrics = df_metrics[df_metrics["Metric"].str.contains("ROUGE")]

# Configuración de estilo bonito
plt.figure(figsize=(8, 5))
bars = plt.bar(df_rouge_metrics["Metric"], df_rouge_metrics["Score"],
               color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'], edgecolor='black')

# Agregar valores arriba de cada barra
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02, f"{height:.2f}", ha='center', va='bottom', fontsize=12)

# Estética del gráfico
plt.ylim(0, 1)
plt.title("ROUGE Scores - Simplification\nAfter A", fontsize=16, fontweight='bold')
plt.ylabel("Score", fontsize=14)
plt.xlabel("Metric", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Ajustar layout para que no se corte nada
plt.tight_layout()

# Mostrar
plt.show()

train_dataset_b = TextDataset(df_b_train, tokenizer)
test_dataset_b = TextDataset(df_b_test, tokenizer)

model = T5ForConditionalGeneration.from_pretrained("./model_after_A")
tokenizer = T5Tokenizer.from_pretrained("./model_after_A")

# mover a device
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

training_args = TrainingArguments(
    output_dir='./results_B_noCL',
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_dir='./logs_B_noCL',
    logging_steps=10,
    learning_rate=5e-4,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_b,
    eval_dataset=test_dataset_b
)

trainer.train()

# Guardar modelo after B no CL
model.save_pretrained("./model_after_B_noCL")
tokenizer.save_pretrained("./model_after_B_noCL")

def run_model(text, prefix):
    input_text = prefix + text
    input_enc = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=128).to(device)
    output = model.generate(input_enc, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True), input_text

model = T5ForConditionalGeneration.from_pretrained("./model_after_B_noCL")
tokenizer = T5Tokenizer.from_pretrained("./model_after_B_noCL")

# mover a device
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

for idx in range(5):
    original_text = df_b_test.iloc[idx]["Segmento"]
    target_text = df_b_test.iloc[idx]["Propuesta"]
    prediction = run_model(original_text, "translate: ")

    prediction_text, model_input = run_model(original_text, "translate: ")  # ojo, tu prefix debería ser este!

    print(f"Ejemplo {idx+1} - Translation")
    print("Input:", model_input)  # opcional, podés mostrarlo si querés
    print("Target (translation):", target_text)
    print("Prediction:", prediction_text)
    print("-" * 80)

for idx in range(30):
    original_text = df_a_test.iloc[idx]["Segmento"]
    target_text = df_a_test.iloc[idx]["Propuesta"]
    prediction, model_input = run_model(original_text, "simplify: ")

    print(f"Ejemplo {idx+1} - Simplificación")
    print("Input:", model_input)  # <- aquí ves el input exacto
    print("Target (simplification):", target_text)
    print("Prediction:", prediction)
    print("-" * 80)

model = T5ForConditionalGeneration.from_pretrained("./model_after_B_noCL")
tokenizer = T5Tokenizer.from_pretrained("./model_after_B_noCL")
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

def run_model(text, prefix):
    input_text = prefix + text
    input_enc = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=128).to(device)
    output = model.generate(input_enc, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True), input_text

import evaluate

rouge = evaluate.load("rouge")

predictions_a = []
references_a = []

for i in range(len(df_a_test)):
    original_text = df_a_test.iloc[i]["Segmento"]
    target_text = df_a_test.iloc[i]["Propuesta"]

    prediction_text, model_input = run_model(original_text, "simplify: ")

    predictions_a.append(prediction_text)
    references_a.append(target_text)

# Calcular ROUGE
results_rouge_a_afterBnoCL = rouge.compute(predictions=predictions_a, references=references_a)

# Mostrar resultados
print(results_rouge_a_afterBnoCL)

# (Opcional) Guardarlo en DataFrame para que te quede ordenado
import pandas as pd

df_metrics_afterBnoCL_testA = pd.DataFrame({
    "Metric": ["ROUGE-1", "ROUGE-2", "ROUGE-L", "ROUGE-Lsum"],
    "Score": [
        results_rouge_a_afterBnoCL["rouge1"],
        results_rouge_a_afterBnoCL["rouge2"],
        results_rouge_a_afterBnoCL["rougeL"],
        results_rouge_a_afterBnoCL["rougeLsum"]
    ]
})

# Mostrar tabla
df_metrics_afterBnoCL_testA

# (Opcional) Exportar
df_metrics_afterBnoCL_testA.to_csv("metrics_afterBnoCL_testA.csv", index=False)

import pandas as pd

# Tabla ordenada de ROUGE → con nombres claros
df_metrics_afterBnoCL_testA = pd.DataFrame({
    "Métrica": ["ROUGE-1", "ROUGE-2", "ROUGE-L", "ROUGE-Lsum"],
    "Valor": [
        round(results_rouge_a_afterBnoCL["rouge1"], 4),
        round(results_rouge_a_afterBnoCL["rouge2"], 4),
        round(results_rouge_a_afterBnoCL["rougeL"], 4),
        round(results_rouge_a_afterBnoCL["rougeLsum"], 4)
    ]
})

# Mostrar la tabla
df_metrics_afterBnoCL_testA




import matplotlib.pyplot as plt

# Gráfico bonito para ROUGE After B no CL → Test A
plt.figure(figsize=(8, 5))
bars = plt.bar(df_metrics_afterBnoCL_testA["Métrica"], df_metrics_afterBnoCL_testA["Valor"],
               color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'], edgecolor='black')

# Agregar valores arriba de cada barra
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02, f"{height:.2f}", ha='center', va='bottom', fontsize=12)

# Estética del gráfico
plt.ylim(0, 1)
plt.title("ROUGE Scores - Simplification\nAfter B no CL", fontsize=16, fontweight='bold')
plt.ylabel("Score", fontsize=14)
plt.xlabel("Metric", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Ajustar layout para que no se corte nada
plt.tight_layout()

# Mostrar
plt.show()

model = T5ForConditionalGeneration.from_pretrained("./model_after_B_noCL")
tokenizer = T5Tokenizer.from_pretrained("./model_after_B_noCL")
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

def run_model(text, prefix):
    input_text = prefix + text
    input_enc = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=128).to(device)
    output = model.generate(input_enc, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True), input_text

import evaluate

# Cargar métrica BLEU
bleu = evaluate.load("bleu")

# Preparar listas
predictions_b = []
references_b = []

# Recorrer Test B
for i in range(len(df_b_test)):
    original_text = df_b_test.iloc[i]["Segmento"]
    target_text = df_b_test.iloc[i]["Propuesta"]

    # OJO → Usamos el prefix correcto
    prediction_text, model_input = run_model(original_text, "translate: ")

    predictions_b.append(prediction_text)
    references_b.append(target_text)

# BLEU espera referencias como listas de listas
references_bleu_b = [[ref] for ref in references_b]

# Calcular BLEU
results_bleu_b_afterBnoCL = bleu.compute(predictions=predictions_b, references=references_bleu_b)

# Mostrar resultados
print(results_bleu_b_afterBnoCL)

# (Opcional) Guardar tabla ordenada → para el paper
df_bleu_afterBnoCL_testB = pd.DataFrame({
    "Métrica": ["BLEU"],
    "Valor": [round(results_bleu_b_afterBnoCL["bleu"], 4)]
})

# Mostrar la tabla
df_bleu_afterBnoCL_testB

"""CONTINUAL LEARNING CL"""

# Asegurarse que Segmento y Propuesta sean str
df_a_train["Segmento"] = df_a_train["Segmento"].astype(str)
df_a_train["Propuesta"] = df_a_train["Propuesta"].astype(str)

# Preparar columnas
df_a_train["prefix"] = "simplify: "
df_a_train["input_text"] = df_a_train["prefix"] + df_a_train["Segmento"]
df_a_train["target_text"] = df_a_train["Propuesta"]


# Asegurarse que Segmento y Propuesta sean str
df_b_train["Segmento"] = df_b_train["Segmento"].astype(str)
df_b_train["Propuesta"] = df_b_train["Propuesta"].astype(str)

# Preparar columnas
df_b_train["prefix"] = "translate: "
df_b_train["input_text"] = df_b_train["prefix"] + df_b_train["Segmento"]
df_b_train["target_text"] = df_b_train["Propuesta"]

print(df_a_train[["input_text", "target_text"]].head(3))
print(df_b_train[["input_text", "target_text"]].head(3))

# Tomar 50% de A para replay
df_a_replay = df_a_train.sample(frac=0.5, random_state=42)

# Concatenar A + B
df_replay = pd.concat([df_a_replay, df_b_train]).reset_index(drop=True)

# Mezclar aleatoriamente
df_replay = df_replay.sample(frac=1.0, random_state=42).reset_index(drop=True)

#Entrenar modelo

train_dataset_replay = TextDataset(df_replay, tokenizer)

model = T5ForConditionalGeneration.from_pretrained("./model_after_A")
tokenizer = T5Tokenizer.from_pretrained("./model_after_A")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

training_args = TrainingArguments(
    output_dir='./results_B_withCL',
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_dir='./logs_B_withCL',
    logging_steps=10,
    learning_rate=5e-4,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_replay,
    eval_dataset=test_dataset_a
)

trainer.train()

model.save_pretrained("./model_after_B_withCL")
tokenizer.save_pretrained("./model_after_B_withCL")

#Imprimir ejemplos de simplificación

model = T5ForConditionalGeneration.from_pretrained("./model_after_B_withCL")
tokenizer = T5Tokenizer.from_pretrained("./model_after_B_withCL")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

def run_model(text, prefix):
    input_text = prefix + text
    input_enc = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=128).to(device)
    output = model.generate(input_enc, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True), input_text


for idx in range(20):
    original_text = df_a_test.iloc[idx]["Segmento"]
    target_text = df_a_test.iloc[idx]["Propuesta"]
    prediction, model_input = run_model(original_text, "simplify: ")

    print(f"Ejemplo {idx+1} - Simplificación")
    print("Input:", model_input)
    print("Target (simplification):", target_text)
    print("Prediction:", prediction)
    print("-" * 80)

# Probar traducción manualmente (Test B)

for idx in range(20):
    original_text = df_b_test.iloc[idx]["Segmento"]
    target_text = df_b_test.iloc[idx]["Propuesta"]
    prediction, model_input = run_model(original_text, "translate: ")

    print(f"Ejemplo {idx+1} - Traducción")
    print("Input:", model_input)  # <- aquí ves el input exacto
    print("Target (translation):", target_text)
    print("Prediction:", prediction)
    print("-" * 80)

model = T5ForConditionalGeneration.from_pretrained("./model_after_B_withCL")
tokenizer = T5Tokenizer.from_pretrained("./model_after_B_withCL")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


rouge = evaluate.load("rouge")

predictions_a_withCL = []
references_a_withCL = []

for i in range(len(df_a_test)):
    original_text = df_a_test.iloc[i]["Segmento"]
    target_text = df_a_test.iloc[i]["Propuesta"]

    prediction_text, model_input = run_model(original_text, "simplify: ")

    predictions_a_withCL.append(prediction_text)
    references_a_withCL.append(target_text)

# Calcular ROUGE
results_rouge_a_withCL = rouge.compute(predictions=predictions_a_withCL, references=references_a_withCL)

# Tabla ordenada
df_metrics_afterBwithCL_testA = pd.DataFrame({
    "Métrica": ["ROUGE-1", "ROUGE-2", "ROUGE-L", "ROUGE-Lsum"],
    "Valor": [
        round(results_rouge_a_withCL["rouge1"], 4),
        round(results_rouge_a_withCL["rouge2"], 4),
        round(results_rouge_a_withCL["rougeL"], 4),
        round(results_rouge_a_withCL["rougeLsum"], 4)
    ]
})


df_metrics_afterBwithCL_testA

import matplotlib.pyplot as plt

# Gráfico bonito para ROUGE After B with CL → Test A
plt.figure(figsize=(8, 5))
bars = plt.bar(df_metrics_afterBwithCL_testA["Métrica"], df_metrics_afterBwithCL_testA["Valor"],
               color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'], edgecolor='black')

# Agregar valores arriba de cada barra
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02, f"{height:.2f}", ha='center', va='bottom', fontsize=12)

# Estética del gráfico
plt.ylim(0, 1)
plt.title("ROUGE Scores - Simplification  \nAfter B with CL", fontsize=16, fontweight='bold')
plt.ylabel("Score", fontsize=14)
plt.xlabel("Metric", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Ajustar layout para que no se corte nada
plt.tight_layout()

# Mostrar
plt.show()

model = T5ForConditionalGeneration.from_pretrained("./model_after_B_withCL")
tokenizer = T5Tokenizer.from_pretrained("./model_after_B_withCL")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


bleu = evaluate.load("bleu")

predictions_b_withCL = []
references_b_withCL = []

for i in range(len(df_b_test)):
    original_text = df_b_test.iloc[i]["Segmento"]
    target_text = df_b_test.iloc[i]["Propuesta"]

    prediction_text, model_input = run_model(original_text, "translate: ")

    predictions_b_withCL.append(prediction_text)
    references_b_withCL.append(target_text)

# BLEU espera referencias como listas de listas
references_bleu_b_withCL = [[ref] for ref in references_b_withCL]

# Calcular BLEU
results_bleu_b_afterBwithCL = bleu.compute(predictions=predictions_b_withCL, references=references_bleu_b_withCL)

# Tabla ordenada
df_bleu_afterBwithCL_testB = pd.DataFrame({
    "Métrica": ["BLEU"],
    "Valor": [round(results_bleu_b_afterBwithCL["bleu"], 4)]
})

df_bleu_afterBwithCL_testB.to_csv("bleu_afterBwithCL_testB.csv", index=False)
df_bleu_afterBwithCL_testB