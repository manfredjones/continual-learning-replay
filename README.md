# Aprendizaje Continuo para la SimplificaciÃ³n de Texto en EspaÃ±ol

Este proyecto investiga el uso de tÃ©cnicas de **Aprendizaje Continuo (Continual Learning)** para mitigar el *olvido catastrÃ³fico* al entrenar modelos de lenguaje de forma secuencial. Se utiliza el modelo **T5-small** y la tÃ©cnica de **Replay** para mantener el rendimiento en tareas previamente aprendidas.

## ğŸ§  DescripciÃ³n del proyecto

- ğŸ§¾ Modelo base: T5-small (Transformers de HuggingFace)  
- ğŸ—‚ï¸ Tareas:
  - A: SimplificaciÃ³n automÃ¡tica de texto en espaÃ±ol
  - B: TraducciÃ³n u otra tarea NLP relacionada
- ğŸ› ï¸ TÃ©cnica aplicada: Replay (almacenamiento y reutilizaciÃ³n de ejemplos anteriores)
- ğŸ“Š MÃ©tricas utilizadas:
  - ROUGE para simplificaciÃ³n
  - BLEU para traducciÃ³n

## ğŸ§ª Herramientas

- Python, PyTorch, HuggingFace
- Jupyter Notebook (compatible con Google Colab)
- Google Drive para almacenamiento de modelos y datos

## ğŸ“ Archivos

- `CL_Simplificacion_T5.ipynb`: Notebook principal con entrenamiento y evaluaciÃ³n

## ğŸ” Resultados

El uso de Replay ayuda a reducir significativamente el olvido catastrÃ³fico al incorporar nuevas tareas, manteniendo el rendimiento en tareas anteriores.

ğŸ“˜ [Ver en Colab](https://colab.research.google.com/drive/1-JQLwsWq12xFTrBg0jpH2B0Uckzi0_iB?usp=sharing)

